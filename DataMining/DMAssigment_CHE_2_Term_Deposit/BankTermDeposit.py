# -*- coding: utf-8 -*-
"""DMAssignmnt.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZJhgYZcM-Nde0zos_sLV7gIZResaLz1h

# Problem Statement:
The Bank Marketing data is related with direct marketing campaigns of a Portuguese banking institution.

*   
The marketing campaigns were based on phone calls. Often, more than one contact to the same client was required, in order to access if the product (bank term deposit) would be (or not) subscribed. 

*   The classification goal is to predict if the client will subscribe a Term Deposit Taken (variable y).



### Predictor / Independent Variables :
1. **Age** : (numeric) 

2. **Job** : type of job (categorical: “admin”, “blue-collar”, “entrepreneur”, “housemaid”, “management”, “retired”, “self-employed”, “services”, “student”, “technician”, “unemployed”, “unknown”) 

3. **Marital Status** : marital status (categorical: “divorced”, “married”, “single”, “unknown”) 

4. **Education** : (categorical: “basic.4y”, “basic.6y”, “basic.9y”, “high.school”, “illiterate”, “professional.course”, “university.degree”, “unknown”) 

5. **Credit Default:** has credit in default? (categorical: “no”, “yes”, “unknown”). 

6. **Housing Loan** : has housing loan? (categorical: “no”, “yes”, “unknown”) 

7. **Personal Loan** : has personal loan? (categorical: “no”, “yes”, “unknown”) 

### Target / Dependent Variable :

1. **Term Deposit Taken** :  has the client subscribed a term deposit ? (binary: “1” means “Yes”, “0” means “No” )
"""

import pandas as pd
import matplotlib.pyplot as plt
import copy
import numpy as np
import seaborn as sns
import pydotplus
import warnings
import operator
from sklearn.model_selection import train_test_split
from sklearn import datasets
from sklearn import svm
from sklearn import preprocessing as preprocess
from sklearn import model_selection
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import cross_validate
from sklearn import metrics
from matplotlib.pyplot import gcf
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz
from sklearn.externals.six import StringIO  
from IPython.display import Image  
from sklearn.metrics import roc_curve
from sklearn.metrics import roc_auc_score
from matplotlib import pyplot
warnings.filterwarnings('ignore')

df = pd.read_excel('Bank Data for case study assignment.xlsx')

# View the First 5 rows of DataFrame :

df.head()

# View the Last 5 rows of DataFrame :

df.tail()

"""#Perform exploratory data analysis"""

df.columns

df.shape

"""## Individual Data Field analysis

### **Age Field - data analysis**
"""

# Binning the Data of Age to make it Categorical :

df['Age'] = pd.cut(df['age'], [10, 20, 30,40, 50, 60, 70, 80, 90], labels=['11-20', '21-30', '31-40', '41-50', '51-60', '61-70', '71-80', '81-90'])

# Unique Age Values :

print("Age - Unique Values :\n\n",          df['Age'].unique(),'\n')
print("Age - No of Unique Values :\n\n",    df['Age'].nunique(),'\n')
# print("Age - Frequency Count Values :\n\n", data['Age'].value_counts())
print("Age - Frequency Count Values :\n\n", df['Age'].value_counts().plot(kind="bar"))
plt.xlabel("Age Groups", labelpad=10)
plt.ylabel("Total Members", labelpad=10)
plt.title("Total Members based on Age Groups", y=1.02);

"""### **Job Field - data analysis**"""

# Unique Job Values :

print("Job - Unique Values :\n\n",          df['job'].unique(),'\n\n')
print("Job - No of Unique Values :\n\n",    df['job'].nunique(),'\n\n')
print("Job - Frequency Count Values :\n\n", df['job'].value_counts().plot(kind="bar"))
plt.xlabel("Job Groups", labelpad=14)
plt.ylabel("Total Jobs", labelpad=14)
plt.title("Total Jobs based on Job Groups", y=1.02);

"""### **Marital Status Field - data analysis**"""

# Unique Marital Status Values :

print("Marital Status - Unique Values :\n\n",          df['marital status '].unique(),'\n\n')
print("Marital Status - No of Unique Values :\n\n",    df['marital status '].nunique(),'\n\n')
print("Marital Status - Frequency Count Values :\n\n", df['marital status '].value_counts().plot(kind="bar"))
plt.xlabel("Marital Status", labelpad=14)
plt.ylabel("Total Status", labelpad=14)
plt.title("Total Status based on Marital Status", y=1);

"""### **Education Field - data analysis**"""

# Unique Education Values :

print("Education - Unique Values :\n\n",               df['education'].unique(),'\n\n')
print("Education - No of Unique Values :\n\n",         df['education'].nunique(),'\n\n')
print("Education - Frequency Count Values :\n\n", df['education'].value_counts().plot(kind="bar"))
plt.xlabel("Education", labelpad=14)
plt.ylabel("Total Count", labelpad=14)
plt.title("Total Count based on Education", y=1.02);

# NOTE : We have Inconsistent 4th Category - 'unknown'.

"""### **Credit Defaul Field - data analysis**"""

# Unique Credit Default Values :

print("Credit Default - Unique Values :\n\n",          df['credit default?'].unique(),'\n\n')
print("Credit Default - No of Unique Values :\n\n",    df['credit default?'].nunique(),'\n\n')
print("Credit Default - Frequency Count Values :\n\n", df['credit default?'].value_counts().plot(kind="bar"))
plt.xlabel("Credit Default", labelpad=14)
plt.ylabel("Total Count", labelpad=14)
plt.title("Total Count based on Credit Default", y=1.02);

"""### **Housing Loan Field - data analysis**"""

# Unique Housing Loan Values :

print("Housing Loan - Unique Values :\n\n",          df['housing loan?'].unique(),'\n\n')
print("Housing Loan - No of Unique Values :\n\n",    df['housing loan?'].nunique(),'\n\n')
print("Housing Loan - Frequency Count Values :\n\n", df['housing loan?'].value_counts().plot(kind="bar"))
plt.xlabel("Housing Loan", labelpad=14)
plt.ylabel("Total Count", labelpad=14)
plt.title("Total Count based on Housing Loan", y=1.02);

# NOTE : We have Inconsistent 3rd Category - 'xxxyy'.

"""### **Personal Loan Field - data analysis**"""

# Unique Personal Loan Values :

print("Personal Loan - Unique Values :\n\n",       df['Personal loan'].unique(),'\n\n')
print("Personal Loan - No of Unique Values :\n\n", df['Personal loan'].nunique(),'\n\n')
print("Personal Loan - Frequency Count Values :\n\n", df['Personal loan'].value_counts().plot(kind="bar"))
plt.xlabel("Personal Loan", labelpad=14)
plt.ylabel("Total Count", labelpad=14)
plt.title("Total Count based on Personal Loan", y=1.02);

"""### **Term Deposit Taken Field - data analysis**"""

# Unique Term Deposit Taken Values :

print("Term Deposit Taken - Unique Values :\n\n",       df['y'].unique(),'\n\n')
print("Term Deposit Taken - No of Unique Values :\n\n", df['y'].nunique(),'\n\n')
print("Term Deposit Taken - Frequency Count Values :\n\n", df['y'].value_counts().plot(kind="bar"))
plt.xlabel("Term Deposit Taken", labelpad=14)
plt.ylabel("Total Count", labelpad=14)
plt.title("Total Count based on Term Deposit Taken", y=1.02);

"""## Combined data Analysis"""

df.groupby(['Age'])['y'].value_counts().unstack().plot(kind='bar')
fig,ax= plt.subplots(3,2,figsize=(15,20))
df.groupby(['job'])['y'].value_counts().unstack().plot(kind='bar',ax=ax[0,0])
df.groupby(['marital status '])['y'].value_counts().unstack().plot(kind='bar',ax=ax[0,1])
df.groupby(['education'])['y'].value_counts().unstack().plot(kind='bar',ax=ax[1,0])
df.groupby(['credit default?'])['y'].value_counts().unstack().plot(kind='bar',ax=ax[1,1])
df.groupby(['housing loan?'])['y'].value_counts().unstack().plot(kind='bar',ax=ax[2,0])
df.groupby(['Personal loan'])['y'].value_counts().unstack().plot(kind='bar',ax=ax[2,1])

"""## Analysis Based on Data exploratory

From the combined data exporatory it is seen that below people have more chances to subscribe term deposit 
1. Age: 31 to 50
2. Job: blue-collar, technician and management
3. Marital status: married
4. Education: secondary
5. Credit default: no
6. Housing loan: yes
7. Personal loan:  no

# Preprocess the data :

### 1. Dropping NA Values:
"""

#Data information before preprocessing
df.info()

# Data frame length before preprocessing
len(df)

# Printing Sum of Missing Values in Each Column :
total_null = df.isnull().sum().sort_values(ascending=False)
print(total_null)

df1 = df[df.isna().any(axis=1)]

df1.index

df3 = df.drop(df1.index.tolist())

# Data information after preprocessing
df3.info()

# Data frame length after preprocessing
len(df3)

"""### Replacing anamoly with max occurence
In Housing Loan Field - data analysis  it is seen that there is an inappropriate value (xxxyy). Replacing with max occurence of the field
"""

df['housing loan?'] = df['housing loan?'].str.replace('xxxyy','yes')

"""# Select Training data, test data"""

label_encode_job = preprocess.LabelEncoder()
label_encode_marital = preprocess.LabelEncoder()
label_encode_eduction = preprocess.LabelEncoder()
label_encode_credit = preprocess.LabelEncoder()
label_encode_housing = preprocess.LabelEncoder()
label_encode_person = preprocess.LabelEncoder()
label_encode_y = preprocess.LabelEncoder()

label_encode_job.fit(df3['job'].tolist())
label_encode_marital.fit(df3['marital status '].tolist())
label_encode_eduction.fit(df3['education'].tolist())
label_encode_credit.fit(df3['credit default?'].tolist())
label_encode_housing.fit(df3['housing loan?'].tolist())
label_encode_person.fit(df3['Personal loan'].tolist())
label_encode_y.fit(df3['y'].tolist())

job_ = label_encode_job.transform(df3['job'].tolist())
marital_ = label_encode_marital.transform(df3['marital status '].tolist())
education_ = label_encode_eduction.transform(df3['education'].tolist())
credit_ = label_encode_credit.transform(df3['credit default?'].tolist())
housing_ = label_encode_housing.transform(df3['housing loan?'].tolist())
person_ = label_encode_person.transform(df3['Personal loan'].tolist())
y_ = label_encode_y.transform(df3['y'].tolist())

df_2 = copy.copy(df3)

#df_2.drop(['Age'], axis=1)
del df_2['Age']

df_2.head()

feature_cols =['age','job','matrial_status ', 'education', 'credit','housing_loan', 'personal_loan']

df_to_use = pd.DataFrame(data=list(zip(df_2.age.tolist(),job_,marital_,education_,credit_,housing_,person_,y_)), 
             columns=['age','job','matrial_status ', 'education', 'credit','housing_loan', 'personal_loan','y'])

y = df_to_use.y
X = df_to_use.drop('y', axis=1)

train_data, test_data, train_y, test_y = model_selection.train_test_split(X, y, test_size=0.2, random_state=13)

train_data[:10]

"""# Correlation analysis"""

plt.figure(figsize=(10,10)) 
sns.heatmap(df_to_use[['age','job','matrial_status ', 'education', 'credit','housing_loan', 'personal_loan','y']].corr(),
            annot=True,cmap ='RdYlBu_r')

"""# Train  and test the model

**Building the classifier with criterion Entropy**
"""

# Building the classifier with criterion Entropy
decision_tree_classifier = DecisionTreeClassifier(criterion="entropy", max_depth=3, random_state=13)
# Training the classifier
decision_tree_classifier.fit(train_data,train_y)

prediction = decision_tree_classifier.predict(test_data)

prediction

# We are calculating accuracy on testing data
accuracy = metrics.accuracy_score(test_y,prediction)

print('Accuracy of the Decision tree with Entropy index : ', accuracy*100)

dot_data = StringIO()
export_graphviz(decision_tree_classifier, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('DecisionTree_Entropy.png')
Image(graph.create_png())

"""**Building the classifier with criterion Gini**"""

decision_tree_classifier = DecisionTreeClassifier(criterion="gini", max_depth=3, random_state=13)
# Training the classifier
decision_tree_classifier.fit(train_data,train_y)

prediction = decision_tree_classifier.predict(test_data)

prediction

#calculating accuracy on testing data
accuracy = metrics.accuracy_score(test_y,prediction)

print('Accuracy of the Decision tree with Gini index : ', accuracy*100)

dot_data = StringIO()
export_graphviz(decision_tree_classifier, out_file=dot_data,  
                filled=True, rounded=True,
                special_characters=True,feature_names = feature_cols,class_names=['0','1'])
graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  
graph.write_png('DecisionTree_Gini.png')
Image(graph.create_png())

"""**Analysis**

Buiding the classifier with criterian entropy and Gini provides the same accuracy.

# Evaluate the model performance
"""

plt = sns.heatmap(pd.DataFrame(metrics.confusion_matrix(test_y,prediction)), annot=True, cmap="YlGnBu" ,fmt='g')
 
  #Confusion Matrix
  confusion_matrix=metrics.confusion_matrix(test_y,prediction)
  print(confusion_matrix)
  
  #Classification Report
  classification_report=metrics.classification_report(test_y,prediction)
  print(classification_report)

# Cross validation using gini index and entropy 15 cv
# ROC and AUC curve plotting]

"""# Cross validation"""

score = model_selection.cross_validate(decision_tree_classifier, train_data, train_y, cv=15, return_estimator=True, n_jobs=-1)

test_score = []
estimator = []
for key, value in score.items():
    if key == 'test_score':
        test_score.append(value)
    if key == 'estimator':
        estimator.append(value)
    else:

      continue

#estimator
estimator[0][3]

max_index, max_value = max(enumerate(test_score), key=operator.itemgetter(1))

max_estimator = estimator[max_index]

max_estimator

predicts = estimator[0][3].predict_proba(test_data)

predicts

# keep probabilities for the positive outcome only
probs = predicts[:, 1]
# calculate AUC
auc = roc_auc_score(test_y, probs)
print('AUC: %.3f' % auc)
# calculate roc curve
fpr, tpr, thresholds = roc_curve(test_y, probs)
# plot no skill
pyplot.plot([0, 1], [0, 1], linestyle='--')
# plot the roc curve for the model
pyplot.plot(fpr, tpr, marker='.')
# show the plot
pyplot.show()

"""# Inference

**Data exploratory**
*  Provided the detailed analysis of each and every attribute of the data set using plots.
*  Found the anamoly in housing loan column with value xxxyy which is handled in data preprocessing.

**Data preprocessing**
* Dropping NA Values: From the given dataset, it is seen that 8 records have empty values. Since it is negligible (< 1%) these entries can be deleted.
*Replacing with max occurence of the field: The anamoly in housing column with value "xxxyy" replaced with maximum occurence value("yes") of the field.

**Data selection, Model creation and testing**
*   From the given data set,  80% data is used for training purpose and remaining 20% is used for testing purpose.
*   Verified the accuracy with both entropy and gini index classifier criterion. 

**Evaluate the model performance**
*  Evaluated the model performance with confusion matrix and classification report.

**Cross Validation**
* Used Cross validate function (sklearn.model_selection.cross_validate) since
1) It allows specifying multiple metrics for evaluation.
2) It returns a dict containing fit-times, score-times (and optionally training scores as well as fitted estimators) in addition to the test score.
* AUC score obtained by using cross validate function for the given data set is 0.607.
"""